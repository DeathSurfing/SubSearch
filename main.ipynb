{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_chunks(text, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Splits a text into chunks of specified word count.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Qdrant (Vector DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_qdrant(collection_name):\n",
    "    \"\"\"\n",
    "    Initializes Qdrant client and creates a collection if it doesn't exist.\n",
    "    \"\"\"\n",
    "    client = QdrantClient(\"localhost\", port=6333)  # Connect to Qdrant server\n",
    "    collections = client.get_collections()\n",
    "    if collection_name not in [c.name for c in collections.collections]:\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=384, distance=Distance.COSINE)  # Adjust size for embedding model\n",
    "        )\n",
    "    return client\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_duplicate_chunk(client, collection_name, file_name, chunk_number):\n",
    "    \"\"\"\n",
    "    Checks if a chunk with the same file name and chunk number already exists in Qdrant.\n",
    "    \"\"\"\n",
    "    response = client.scroll(\n",
    "        collection_name=collection_name,\n",
    "        scroll_filter=Filter(\n",
    "            must=[\n",
    "                FieldCondition(key=\"file_name\", match=MatchValue(value=file_name)),\n",
    "                FieldCondition(key=\"chunk_number\", match=MatchValue(value=chunk_number))\n",
    "            ]\n",
    "        ),\n",
    "        limit=1  # Only need to check if any match exists\n",
    "    )\n",
    "    return len(response.points) > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get keywords from Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_ollama(chunk_text, model=\"llama3.2:1b\"):\n",
    "    \"\"\"\n",
    "    Uses the Ollama Python package to generate keywords for a given text chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generate a response from Ollama\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=f\"Extract the main keywords from the following text:\\n\\n{chunk_text}\\n\\nKeywords:\"\n",
    "        )\n",
    "        # Return the keywords (clean up response)\n",
    "        return response[\"response\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get keywords from Ollama: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_and_save_to_qdrant(folder_path, collection_name, model_name='all-MiniLM-L6-v2'):\n",
    "    model = SentenceTransformer(model_name)  # Embedding model\n",
    "    client = initialize_qdrant(collection_name)\n",
    "    point_id = 0  # Unique ID for chunks\n",
    "    \n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    chunks = get_text_chunks(content)\n",
    "                    \n",
    "                    for idx, chunk in enumerate(chunks, start=1):\n",
    "                        # Skip duplicates\n",
    "                        if is_duplicate_chunk(client, collection_name, os.path.basename(file_path), idx):\n",
    "                            print(f\"Skipping duplicate: {os.path.basename(file_path)} - Chunk {idx}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Generate embedding\n",
    "                        embedding = model.encode(chunk)\n",
    "                        \n",
    "                        # Get keywords from Ollama\n",
    "                        keywords = get_keywords_from_ollama(chunk)\n",
    "                        \n",
    "                        # Metadata\n",
    "                        metadata = {\n",
    "                            \"file_name\": os.path.basename(file_path),\n",
    "                            \"chunk_number\": idx,\n",
    "                            \"content\": chunk,\n",
    "                            \"keywords\": keywords\n",
    "                        }\n",
    "                        \n",
    "                        # Insert into Qdrant\n",
    "                        client.upsert(\n",
    "                            collection_name=collection_name,\n",
    "                            points=[\n",
    "                                PointStruct(id=point_id, vector=embedding.tolist(), payload=metadata)\n",
    "                            ]\n",
    "                        )\n",
    "                        print(f\"Inserted: {metadata}\")\n",
    "                        point_id += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = \"\"\n",
    "collection_name = \"\"\n",
    "process_files_and_save_to_qdrant(folder_path, collection_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
